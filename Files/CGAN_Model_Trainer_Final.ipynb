{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C67bQ74f9ZE8"
   },
   "source": [
    "# This notebook is for training of our CGAN model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2jMSkjXTjwD1"
   },
   "source": [
    "Initially we will import all the required libraries which are necessary for training of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fS5ejzJdjwD2",
    "outputId": "f827df82-dcc9-4f40-f583-b6b474380ffd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import load\n",
    "from numpy import zeros\n",
    "from numpy import ones\n",
    "from numpy import asarray\n",
    "from numpy import append\n",
    "from numpy.random import random\n",
    "from numpy.random import randint\n",
    "from numpy.random import shuffle\n",
    "import time\n",
    "import os\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import Adamax\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Reshape\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import Conv2DTranspose\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from matplotlib import patheffects as path_effects\n",
    "import collections\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow import get_logger as log\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aGt-Nul2jwD7"
   },
   "source": [
    "We will mount our google drive to our google colab platform so that we can access files and the save the results in the drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "N2TRZpeE9rNU",
    "outputId": "ef0dfc27-b904-4f4d-96fa-68605d1460ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "os.chdir(\"/content/drive/My Drive/deeplearning/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ieJBJTqG-gpj"
   },
   "outputs": [],
   "source": [
    "#We will set our flags and turn off the warnings for a precise output. Tensorflow and Keras are both very good at giving warnings when syntax being used is out of date, dimensions do not match, or features (such as trainable=True) are not used as required. The problem is you sometimes have to run through many warnings before seeing the impact of the issue. In debugging circumstances, being able to shut off warnings can be helpful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sCHO27dL-gsS"
   },
   "outputs": [],
   "source": [
    "#  SET YOUR FLAGS\n",
    "qErrorHide = False\n",
    "if qErrorHide:\n",
    "    print(\"\\n***REMEMBER:  WARNINGS turned OFF***\\n***REMEMBER:  WARNINGS turned OFF***\\n\")\n",
    "    log().setLevel('ERROR')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8V2sNiZGjwEL"
   },
   "source": [
    "This is a very important step. As we need a lot of time for our training and there could be many interruptions while training so we will specifiy if we want to start training freshly or use an already existing existing model and continue training from that model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1DAtJ0X_-gvD"
   },
   "outputs": [],
   "source": [
    "#    INDICATE IF STARTING FRESH OR CONTINUING FROM PREVIOUS RUN\n",
    "qRestart = False\n",
    "if qRestart:\n",
    "    epochs_done = 155\n",
    "    epochs_goal = 200\n",
    "else:\n",
    "    epochs_done = 0\n",
    "    epochs_goal = 200\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksFGWH1KjwES"
   },
   "source": [
    "In this section, we will develop a GAN for the faces dataset that we have prepared. The first step is to define the models.\n",
    "The best way to design models in Keras to have multiple inputs is by using the Functional API, as opposed to the Sequential API . We will use the functional API to implement the discriminator, generator, and the composite model.\n",
    "\n",
    "Starting with the discriminator model, a new second input is defined that takes an integer for the class label of the image. This has the effect of making the input image conditional on the provided class label.\n",
    "\n",
    "The class label is then passed through an Embedding layer with the size of 8. This means that each of the 4 classes for the CelebA dataset (0 through 3) will map to a different 8-element vector representation that will be learned by the discriminator model.\n",
    "\n",
    "The output of the embedding is then passed to a fully connected layer with a linear activation. Importantly, the fully connected layer has enough activations that can be reshaped into one channel of a 64x64 image. The activations are reshaped into single 64x64 activation map and concatenated with the input image. This has the effect of looking like a two-channel input image to the next convolutional layer.\n",
    "\n",
    "The define_discriminator() below implements this update to the discriminator model. The parameterized shape of the input image is also used after the embedding layer to define the number of activations for the fully connected layer to reshape its output. The number of classes in the problem is also parameterized in the function and set.\n",
    "\n",
    "We will use a functional modelling of keras while using the embedding layers for labels. It is implemented as a modest convolutional neural network using best practices for GAN design such as using the LeakyReLU activation function with a slope of 0.2, using a 2 × 2 stride to downsample, and the Adamax version of stochastic gradient descent with a learning rate of 0.0007. While Adam optimizers are generally used, Adamax is recommended when there are embeddings. The discriminator model takes as input one 64 × 64 color image and a class label as embedded vector and outputs a binary prediction as to whether the image is real (class = 1) or fake (class = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FJEpq7ZT-g1b"
   },
   "outputs": [],
   "source": [
    "# define the standalone discriminator model\n",
    "def define_discriminator(in_shape=(64,64,3), n_classes=4):\n",
    "    print(\"**********  ENTERED discriminator  *****************\")\n",
    "    ##### foundation for labels\n",
    "    in_label = Input(shape=(1,))\n",
    "    embedding_layer = Embedding(n_classes, 8)\n",
    "    # embedding_layer.trainable = False\n",
    "    li = embedding_layer (in_label)\n",
    "    n_nodes = in_shape[0] * in_shape[1]\n",
    "    print(\">>embedding>> in_shape[0], in_shape[1], n_nodes: \", in_shape[0], in_shape[1], n_nodes)\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((in_shape[0], in_shape[1], 1))(li)\n",
    "    # image input\n",
    "    dropout = 0.1\n",
    "    in_image = Input(shape=in_shape)\n",
    "    print(\"\\nin_image: \", in_image)\n",
    "    # concat label as a channel\n",
    "    merge = Concatenate()([in_image, li])\n",
    "    print(\"\\nmerge.shape: \", merge.shape)\n",
    "    # sample to 64x64\n",
    "    fe = Conv2D(128, (5,5), padding='same')(merge)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    fe = Dropout(dropout)(fe)\n",
    "    print(\"fe.shape: \", fe.shape)\n",
    "    # downsample to 32x32\n",
    "    fe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # fe = Dropout(dropout)(fe)\n",
    "    print(\"fe.shape: \", fe.shape)\n",
    "    # downsample to 16x16\n",
    "    fe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # fe = Dropout(dropout)(fe)\n",
    "    print(\"fe.shape: \", fe.shape)\n",
    "    # downsample to 8x8\n",
    "    fe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # fe = Dropout(dropout)(fe)\n",
    "    print(\"fe.shape: \", fe.shape)\n",
    "    # downsample to 4x4\n",
    "    fe = Conv2D(128, (5,5), strides=(2,2), padding='same')(fe)\n",
    "    fe = LeakyReLU(alpha=0.2)(fe)\n",
    "    # fe = Dropout(dropout)(fe)\n",
    "    print(\"fe.shape: \", fe.shape)\n",
    "    # flatten feature maps\n",
    "    fe = Flatten()(fe)\n",
    "    # fe = Dropout(dropout)(fe)\n",
    "    print(\"fe flatten shape: \", fe.shape)\n",
    "    # output\n",
    "    out_layer = Dense(1, activation='sigmoid')(fe)\n",
    "    print(\"out_layer.shape: \", out_layer.shape)\n",
    "    # define model\n",
    "    model = Model([in_image, in_label], out_layer)\n",
    "    print(\"\\nmodel: \", model)\n",
    "    # compile model\n",
    "    opt = Adamax(lr=0.00007, beta_1=0.08, beta_2=0.999, epsilon=10e-8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n",
    "    print(\"\\nembedding_layer.get_weights(): \\n\",embedding_layer.get_weights())\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='cgan/discriminator_model1.png')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cY8TLIpgjwEW"
   },
   "source": [
    "Next, the generator model must be updated to take the class label. This has the effect of making the point in the latent space conditional on the provided class label.\n",
    "\n",
    "As in the discriminator, the class label is passed through an embedding layer to map it to a unique 8-element vector and is then passed through a fully connected layer with a linear activation before being resized. In this case, the activations of the fully connected layer are resized into a single 5x5 feature map. This is to match the 5x5 feature map activations of the unconditional generator model. The new 5x5 feature map is added as one more channel to the existing 128, resulting in 129 feature maps that are then upsampled as in the prior model.\n",
    "\n",
    "The define_generator() function below implements this, again parameterizing the number of classes as we did with the discriminator model.\n",
    "\n",
    "The generator model takes as input a point in the latent space and embedded labels, and outputs a single 64 × 64 color image. This is achieved by using a fully connected layer to interpret the point in the latent space and provide sufficient activations that can be reshaped into many different (in this case 128) of a low-resolution version of the output image (e.g. 5 × 5). This is then upsampled four times, doubling the size and quadrupling the area of the activations each time using transpose convolutional layers. The model uses best practices such as the LeakyReLU activation, a kernel size that is a factor of the stride size, and a hyperbolic tangent (Tanh) activation function in the output layer. The define generator() function below defines the generator model but intentionally does not compile it as it is not trained directly, then returns the model. The size of the latent space is parameterized as a function argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xxbzjYhKFG4K"
   },
   "outputs": [],
   "source": [
    "# define the standalone generator model\n",
    "def define_generator(latent_dim, n_classes=4):\n",
    "    print(\"**********  ENTERED generator  *****************\")\n",
    "    ##### foundation for labels\n",
    "    in_label = Input(shape=(1,))\n",
    "    embedding_layer = Embedding(n_classes, 8)\n",
    "    embedding_layer.trainable = True\n",
    "    li = embedding_layer (in_label)\n",
    "    n_nodes = 4 * 4\n",
    "    li = Dense(n_nodes)(li)\n",
    "    li = Reshape((4 , 4, 1))(li)\n",
    "    print(\"generator...  n_nodes, li.shape: \", n_nodes, li.shape)\n",
    "    ##### foundation for 4x4 image\n",
    "    in_lat = Input(shape=(latent_dim,))\n",
    "    n_nodes = 128 * 4 * 4\n",
    "    genX = Dense(n_nodes)(in_lat)\n",
    "    genX = LeakyReLU(alpha=0.2)(genX)\n",
    "    genX = Reshape((4, 4, 128))(genX)\n",
    "    dropout = 0.1\n",
    "    print(\"genX.shape: \", genX.shape)\n",
    "    ##### merge image gen and label input\n",
    "    merge = Concatenate()([genX, li])\n",
    "    print(\"merge.shape: \", merge.shape)\n",
    "    ##### create merged model\n",
    "    # upsample to 8x8\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(merge)\n",
    "    print(\"gen after CV2DT.shape: \", gen.shape)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    gen = Dropout(dropout)(gen)\n",
    "    print(\"gen.shape: \", gen.shape)\n",
    "    # upsample to 16x16\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    print(\"gen.shape: \", gen.shape)\n",
    "    # upsample to 32x32\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    print(\"gen.shape: \", gen.shape)\n",
    "    # upsample to 64x64\n",
    "    gen = Conv2DTranspose(128, (4,4), strides=(2,2), padding='same')(gen)\n",
    "    gen = LeakyReLU(alpha=0.2)(gen)\n",
    "    print(\"gen.shape: \", gen.shape)\n",
    "    # output layer 64x64x3\n",
    "    out_layer = Conv2D(3, (5,5), activation='tanh', padding='same')(gen)\n",
    "    print(\"out_layer.shape: \", out_layer.shape)\n",
    "    # define model\n",
    "    model = Model(inputs=[in_lat, in_label], outputs=out_layer)\n",
    "    opt = Adamax(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
    "    model.compile(loss=['binary_crossentropy'], optimizer=opt)\n",
    "    print(\"\\nembedding_layer.get_weights(): \\n\",embedding_layer.get_weights())\n",
    "    model.summary()\n",
    "    # plot_model(model, to_file='generator_model1.png')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eBoxnfJ3jwEZ"
   },
   "source": [
    "Finally, the composite GAN model requires updating. A GAN model can be defined that combines both the generator model and the discriminator model into one larger model. This larger model will be used to train the model weights in the generator, using the output and error calculated by the discriminator model. The discriminator model is trained separately, and as such, the model weights are marked as not trainable in this larger GAN model to ensure that only the weights of the generator model are updated. This change to the trainability of the discriminator weights only has an effect when training the combined GAN model, not when training the discriminator standalone.\n",
    "\n",
    "The new GAN model will take a point in latent space as input and a class label and generate a prediction of whether input was real or fake, as before.\n",
    "\n",
    "Using the functional API to design the model, it is important that we explicitly connect the image generated output from the generator as well as the class label input, both as input to the discriminator model. This allows the same class label input to flow down into the generator and down into the discriminator.\n",
    "\n",
    "\n",
    "\n",
    "This larger GAN model takes as input a point in the latent space, uses the generator model to generate an image, which is fed as input to the discriminator model, then output or classified as real or fake. The define gan() function below implements this, taking the already-defined generator and discriminator models as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LsX56b2c-g4J"
   },
   "outputs": [],
   "source": [
    "# define the combined generator and discriminator model, for updating the generator\n",
    "def define_gan(g_model, d_model):\n",
    "    print(\"**********  ENTERED gan  *****************\")\n",
    "    # make weights in the discriminator not trainable\n",
    "    d_model.trainable = False\n",
    "    # get noise and label inputs from generator model\n",
    "    gen_noise, gen_label = g_model.input\n",
    "    # get image output from the generator model\n",
    "    gen_output = g_model.output\n",
    "    # connect image output and label input from generator as inputs to discriminator\n",
    "    gan_output = d_model([gen_output, gen_label])\n",
    "    # define gan model as taking noise and label and outputting a classification\n",
    "    model = Model([gen_noise, gen_label], gan_output)\n",
    "    # compile model\n",
    "    opt = Adamax(lr=0.0002, beta_1=0.5, beta_2=0.999, epsilon=10e-8)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=opt)\n",
    "    model.summary()\n",
    "    plot_model(model, to_file='cgan/gan_model1.png')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bUy_7kEgjwEb"
   },
   "source": [
    "There are circumstances where we want to insure that a generated image has particular characteristics, such as a face being  attractive, selecting a particular gender, and having facial features such as high cheek bones and large lips. Looking into the near future, it will be possible to create realistic GAN generated images of models wearing fashionable clothing, with specific expressions, and poses for catalogues. In this project, we could enter in the features: attractive, female, high cheek bones, and large lips in order to get many faces for fashion models.\n",
    "\n",
    "There were three parts to this process:\n",
    "\n",
    "selecting a subset of faces (only those identified as being \"attractive\"): Details of the process are discussed in data preprocessing.\n",
    "identifying the characteristics or attributes to be used and their probabilities in the population of images:\n",
    "..... 0 = featured as attractive and female and not high cheek bone and not large lips\n",
    "..... 1 = featured as attractive and male\n",
    "..... 2 = featured as attractive and female and high cheek bone\n",
    "..... 3 = featured as attractive and female and not high cheek bone and large lips\n",
    "setting up the cGAN so that it will generate and save faces based on the features (embeddings/labels) associated with an image.\n",
    "\n",
    "There are four kinds of embedding and the identity of the embedding (0 thru 3) is included in the generated face. In many ways, those faces identified as being 0 are \"female without high cheeck bones and without large lips\". Those faces identified as 1 (male), are clearly male. Those faces identifed as 2 are female with high cheek bones. Feature 3 identifies those faces which supposedly have large lips. The labels (0 thru 3) are added when creating the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dDHR6Frs-g69"
   },
   "outputs": [],
   "source": [
    "# assign categories\n",
    "def assign_categs(df, lenrows):\n",
    "    print(\"\\n*****  ATTRIBUTES: \\n\", df.mean())\n",
    "\n",
    "    face_male = df['Male']\n",
    "    print(\"face_male: \", face_male.mean())\n",
    "    face_male = np.where(face_male > 0, 1, face_male)\n",
    "    print(\"face_male: \", face_male.mean())\n",
    "\n",
    "    face_high_cheekbones = df['High_Cheekbones']\n",
    "    print(\"face_high_cheekbones: \", face_high_cheekbones.mean())\n",
    "    face_high_cheekbones = np.where(face_high_cheekbones > 0, 1, face_high_cheekbones)\n",
    "    print(\"face_high_cheekbones: \", face_high_cheekbones.mean())\n",
    "\n",
    "    face_big_lips = df['Big_Lips']\n",
    "    print(\"face_big_lips: \", face_big_lips.mean())\n",
    "    face_big_lips = np.where(face_big_lips > 0, 1, face_big_lips)\n",
    "    print(\"face_big_lips: \", face_big_lips.mean())\n",
    "\n",
    "    # replace vectors with category value\n",
    "    categs = np.zeros(lenrows, dtype=int)\n",
    "    print(\"categ.mean()): \", categs.mean())\n",
    "    categs = np.where(face_male > 0, 1, categs)\n",
    "    print(\"add face_male: categs.mean()): \", categs.mean())\n",
    "    categs = np.where((face_high_cheekbones > 0)&(categs==0), 2, categs)\n",
    "    print(\"add high_cheekbones: categs.mean()): \", categs.mean())\n",
    "    categs = np.where((face_big_lips > 0)&(categs==0), 3, categs)\n",
    "    print(\"add big lips: categs.mean()): \", categs.mean())\n",
    "    print(\"\\ncategs: \\n\", categs)\n",
    "    return categs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "850vg52a-g9z"
   },
   "outputs": [],
   "source": [
    "def get_cumProbs(freqCategs, categs):\n",
    "    freqLists = [freqCategs[i][1] for i in range(len(freqCategs))]\n",
    "    freqListX = asarray(freqLists, dtype=np.float32)\n",
    "    print(\"freqListX: \", freqListX)\n",
    "    print(\"len(categs): \", len(categs))\n",
    "    cumProbs = freqListX/len(categs)\n",
    "    print(\"cumProbs: \", cumProbs)\n",
    "    cumProbs = append((0.0),cumProbs)\n",
    "    for i in range(len(cumProbs)-1):\n",
    "        cumProbs[i+1]=cumProbs[i]+cumProbs[i+1]\n",
    "    print(\"cumProbs: \", cumProbs)\n",
    "    return cumProbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9TS5UG4ujwEh"
   },
   "source": [
    "Now that we have defined the GAN model, we need to train it. But, before we can train the model, we require input data. The first step is to load and scale the pre-processed faces dataset. The saved NumPy array can be loaded, as we did in the previous section, then the pixel values must be scaled to the range [-1,1] to match the output of the generator model. The load real samples() function below implements this, returning the loaded and scaled image data ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OFMcrxU1-hAh"
   },
   "outputs": [],
   "source": [
    "def load_real_samples():\n",
    "    # load the face dataset\n",
    "    data = load('img_align_celeba_attractive_face.npz')\n",
    "    X = data['arr_0']\n",
    "    # convert from unsigned ints to floats\n",
    "    X = X.astype('float32')\n",
    "    # scale from [0,255] to [-1,1]\n",
    "    X = (X - 127.5) / 127.5\n",
    "    data = pd.read_csv('list_attr_celeba.csv', nrows=100000)\n",
    "    data = data.drop(\"image_id\",axis=1)\n",
    "    ids  = load('ids_align_celeba_attractive_face.npz')\n",
    "    idsX = ids['arr_0']\n",
    "    dataX = list()\n",
    "    for i,id in enumerate(idsX):\n",
    "        dataVal = data[id:id+1].values\n",
    "        dataVal = np.where(dataVal==-1, 0, dataVal)\n",
    "        dataX.append(dataVal)\n",
    "    cols = data.columns\n",
    "    lencols = len(cols)\n",
    "    print(\"cols: \", cols)\n",
    "    lenrows = len(dataX)\n",
    "    dataVals = asarray(dataX[0:]).reshape((lenrows,lencols),)\n",
    "    df = pd.DataFrame(data=dataVals,columns=cols)\n",
    "    pd.options.display.float_format = '{:,.3f}'.format\n",
    "    categs = assign_categs(df, lenrows)\n",
    "    freqCategs = list(collections.Counter(sorted(categs)).items())\n",
    "    print(\"freqCategs: \", freqCategs)\n",
    "    cumProbs = get_cumProbs(freqCategs, categs)\n",
    "    return [X, categs], cumProbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "37qIB1DcjwEk"
   },
   "source": [
    "We will require one batch (or a half batch) of real images from the dataset each update to the GAN model. A simple way to achieve this is to select a random sample of images from the dataset each time. The generate real samples() function below implements this, taking the prepared dataset as an argument, selecting and returning a random sample of face images and their corresponding class label for the discriminator, specifically class = 1, indicating that they are real images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pVYsukSi-hDT"
   },
   "outputs": [],
   "source": [
    "# select real samples\n",
    "def generate_real_samples(dataset, n_samples):\n",
    "    # split into images and labels\n",
    "    images, labels = dataset\n",
    "    # choose random instances\n",
    "    ix = randint(0, images.shape[0], n_samples)\n",
    "    # retrieve selected images\n",
    "    X, labels = images[ix], labels[ix]\n",
    "    # generate 'real' class labels (1)\n",
    "    y = ones((n_samples, 1))\n",
    "    return [X, labels], y\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hCFjo65NjwEm"
   },
   "source": [
    "Next, we need inputs for the generator model. These are random points from the latent space, specifically Gaussian distributed random variables. The generate latent points() function implements this, taking the size of the latent space as an argument and the number of points required and returning them as a batch of input samples for the generator model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AgVy1jX_-hGR"
   },
   "outputs": [],
   "source": [
    "# generate points in latent space as input for the generator\n",
    "def generate_latent_points(latent_dim, n_samples, cumProbs, n_classes=4):\n",
    "    # print(\"generate_latent_points: \", latent_dim, n_samples)\n",
    "    initX = -3.0\n",
    "    rangeX = 2.0*abs(initX)\n",
    "    stepX = rangeX / (latent_dim * n_samples)\n",
    "    x_input = asarray([initX + stepX*(float(i)) for i in range(0,latent_dim * n_samples)])\n",
    "    shuffle(x_input)\n",
    "    # generate points in the latent space\n",
    "    z_input = x_input.reshape(n_samples, latent_dim)\n",
    "    randx = random(n_samples)\n",
    "    labels = np.zeros(n_samples, dtype=int)\n",
    "    for i in range(n_classes):\n",
    "        labels = np.where((randx >= cumProbs[i]) & (randx < cumProbs[i+1]), i, labels)\n",
    "    return [z_input, labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JcMfkIGtjwEp"
   },
   "source": [
    "Next, we need to use the points in the latent space as input to the generator in order to generate new images. The generate fake samples() function below implements this, taking the generator model and size of the latent space as arguments, then generating points in the latent space and using them as input to the generator model. The function returns the generated images and their corresponding class label for the discriminator model, specifically class = 0 to indicate they are fake or generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6NeVSllo-hJB"
   },
   "outputs": [],
   "source": [
    "# use the generator to generate n fake examples, with class labels\n",
    "def generate_fake_samples(generator, latent_dim, n_samples, cumProbs):\n",
    "    # generate points in latent space\n",
    "    z_input, labels_input = generate_latent_points(latent_dim, n_samples, cumProbs)\n",
    "    # predict outputs\n",
    "    images = generator.predict([z_input, labels_input])\n",
    "    # create class labels\n",
    "    y = zeros((n_samples, 1))\n",
    "    return [images, labels_input], y\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pn4bY0N1jwEs"
   },
   "source": [
    "The save plot() is called to create and save a plot of the generated images, and then the model is saved to a file. It's helpful if the image has a label stamped on it so you can see, at a glance, whether or not the embedding matches what you believe ought to be features of the generated image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fcbc1iUxCmjw"
   },
   "outputs": [],
   "source": [
    "# create and save a plot of generated images\n",
    "def save_plot(examples, labels, epoch, n=10):\n",
    "    # scale from [-1,1] to [0,1]\n",
    "    examples = (examples + 1) / 2.0\n",
    "    # plot images\n",
    "    for i in range(n * n):\n",
    "        # define subplot\n",
    "        fig = plt.subplot(n, n, 1 + i)\n",
    "        strLabel = str(labels[i])\n",
    "        # turn off axis\n",
    "        fig.axis('off')\n",
    "        fig.text(8.0,20.0,strLabel, fontsize=6, color='white')\n",
    "        # plot raw pixel data\n",
    "        fig.imshow(examples[i])\n",
    "    # save plot to file\n",
    "    filename = 'results/generated_plots/generated_plots_e%03d.png' % (epoch+1)\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L8HIcVecCmmf"
   },
   "outputs": [],
   "source": [
    "def save_real_plots(dataset, nRealPlots = 5, n=10, n_samples=100):\n",
    "    # plot images\n",
    "    for epoch in range(nRealPlots):\n",
    "        if epoch%5==0:\n",
    "            print(\"real_plots: \", epoch)\n",
    "        # prepare real samples\n",
    "        [X_real, labels], y_real = generate_real_samples(dataset, n_samples)\n",
    "        # scale from [-1,1] to [0,1]\n",
    "        X_real = (X_real + 1) / 2.0\n",
    "        for i in range(n * n):\n",
    "            # define subplot\n",
    "            fig = plt.subplot(n, n, 1 + i)\n",
    "            strLabel = str(labels[i])\n",
    "            # fig.title = strLabel\n",
    "            # turn off axis\n",
    "            fig.axis('off')\n",
    "            fig.text(8.0,20.0,strLabel, fontsize=6, color='white')\n",
    "            # plot raw pixel data\n",
    "            fig.imshow(X_real[i])\n",
    "        # save plot to file\n",
    "        filename = 'results/real_plots/real_plot_e%03d.png' % (epoch+1)\n",
    "        plt.savefig(filename)\n",
    "        plt.close()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xUDLZbpdjwEy"
   },
   "source": [
    "After every 5 training epochs, the summarize performance() function is called. There is currently no reliable way to automatically evaluate the quality of generated images. Therefore, we must generate images periodically during training and save the model at these times. This both provides a checkpoint that we can later load and use to generate images, and a way to safeguard against the training process failing, which can happen. Below defines the summarize performance() and save plot() functions. The summarize performance() function generates samples and evaluates the performance of the discriminator on real and fake samples. The classification accuracy is reported and might provide insight into model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75oyYGcMCmpd"
   },
   "outputs": [],
   "source": [
    "# evaluate the discriminator, plot generated images, save generator model\n",
    "def summarize_performance(epoch, g_model, d_model, gan_model, dataset, latent_dim, n_samples=100):\n",
    "    # prepare real samples\n",
    "    [X_real, labels_real], y_real = generate_real_samples(dataset, n_samples)\n",
    "    # evaluate discriminator on real examples\n",
    "    _, acc_real = d_model.evaluate([X_real, labels_real], y_real, verbose=0)\n",
    "    # prepare fake examples\n",
    "    [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, n_samples, cumProbs)\n",
    "    # evaluate discriminator on fake examples\n",
    "    _, acc_fake = d_model.evaluate([X_fake, labels], y_fake, verbose=0)\n",
    "    # summarize discriminator performance\n",
    "    print('>Accuracy real: %.0f%%, fake: %.0f%%' % (acc_real*100, acc_fake*100))\n",
    "    # save plot\n",
    "    save_plot(X_fake, labels, epoch)\n",
    "    # save the generator model tile file\n",
    "    filename = 'results/models/generator_model_%03d.h5' % (epoch+1)\n",
    "    g_model.save(filename)\n",
    "    filename = 'results/models/generator_model_gan%03d.h5' % (epoch+1)\n",
    "    gan_model.save(filename)\n",
    "    filename = 'results/models/generator_model_dis%03d.h5' % (epoch+1)\n",
    "    d_model.trainable = True\n",
    "    for layer in d_model.layers:\n",
    "        layer.trainable = True\n",
    "    d_model.save(filename)\n",
    "    d_model.trainable = False\n",
    "    for layer in d_model.layers:\n",
    "        layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OBGEsNuhCmsT"
   },
   "outputs": [],
   "source": [
    "def restart(epochs_done):\n",
    "    # gen_weights = array(model.get_weights())\n",
    "    print(\"****  PULLING IN EPOCH: \", epochs_done)\n",
    "    filename = 'results/models/generator_model_dis%03d.h5' % (epochs_done)\n",
    "    d_model = load_model(filename, compile=True)\n",
    "    d_model.trainable = True\n",
    "    for layer in d_model.layers:\n",
    "        layer.trainable = True\n",
    "    d_model.summary()\n",
    "    filename = 'results/models/generator_model_%03d.h5' % (epochs_done)\n",
    "    g_model = load_model(filename, compile=True)\n",
    "    g_model.summary()\n",
    "    gan_model = define_gan(g_model, d_model)\n",
    "    gan_model.summary()\n",
    "    return d_model, g_model, gan_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UEh4QD6LjwE3"
   },
   "source": [
    "We are now ready to fit the GAN models. The model is fit for 100 training epochs, which is arbitrary, as the model begins generating plausible faces after perhaps the first few epochs. A batch size of 128 samples is used, and each training epoch involves 50000/64 or about 781 batches of real and fake samples and updates to the model. First, the discriminator model is updated for a half batch of real samples, then a half batch of fake samples, together forming one batch of weight updates. The generator is then updated via the combined GAN model. Importantly, the class label is set to 1 or real for the fake samples. This has the effect of updating the generator toward getting better at generating real samples on the next batch. The train() function below implements this, taking the defined models, dataset, and size of the latent dimension as arguments and parameterizing the number of epochs and batch size with default arguments.\n",
    "\n",
    "The following programming fragment also illustrates an approach which often prevents a stream from mode collapse. It depends on having captured disciminator weights, generator weights, and gan weights either during initialization or later in the process when all model losses are within bounds. The definition of model loss bounds are arbitrary but reflect expert opinion about when losses are what might be expected and when they are clearly much too high or much too low. Reasonable discriminator and generator losses are between 0.1 and 1.0, and their arbitrary bounds are set to between 0.001 and 2.0. Reasonable gan losses are between 0.2 and 2.0 and their arbitrary bounds are set to 0.01 and 4.5.\n",
    "\n",
    "What happens then is discriminator, generator, and gan weights are collected when all three losses are \"reasonable\". When an individual model's loss goes out of bounds, then the last collected weights for that particular model are replaced, leaving the other model weights are they are, and the process moves forward. The process stops when mode collapse appears to be unavoidable even when model weights are replaced. This is identified when a particular set of model weights continue to be reused but repeatedly result in out of bound model losses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgcZSpAID2bt"
   },
   "outputs": [],
   "source": [
    "# train the generator and discriminator\n",
    "def train(g_model, d_model, gan_model, dataset, latent_dim, epochs_goal=200, n_batch=128, epochs_done=1):\n",
    "    nTryAgains = 0\n",
    "    nTripsOnSameSavedWts = 0\n",
    "    nSaves = 0\n",
    "    bat_per_epo = int(dataset[0].shape[0] / n_batch)\n",
    "    half_batch = int(n_batch / 2)\n",
    "    d_trainable_weights = np.array(d_model.get_weights())\n",
    "    g_trainable_weights = np.array(g_model.get_weights())\n",
    "    gan_trainable_weights = np.array(gan_model.get_weights())\n",
    "    now = time.time()\n",
    "    ij = 0\n",
    "    ijSave = -100\n",
    "    # manually enumerate epochs\n",
    "    for i in range(epochs_done, epochs_goal):\n",
    "        # enumerate batches over the training set\n",
    "        for j in range(bat_per_epo):\n",
    "            ij+=1\n",
    "            # get randomly selected 'real' samples\n",
    "            [X_real, labels_real], y_real = generate_real_samples(dataset, half_batch)\n",
    "            qDebug=False\n",
    "            # update discriminator model weights\n",
    "            dis_loss, _ = d_model.train_on_batch([X_real, labels_real], y_real)\n",
    "            [X_fake, labels], y_fake = generate_fake_samples(g_model, latent_dim, half_batch, cumProbs)\n",
    "            gen_loss, _ = d_model.train_on_batch([X_fake, labels], y_fake)\n",
    "            [z_input, labels_input] = generate_latent_points(latent_dim, n_batch, cumProbs)\n",
    "            # create inverted labels for the fake samples\n",
    "            y_gan = ones((n_batch, 1))\n",
    "            # update the generator via the discriminator's error\n",
    "            gan_loss = gan_model.train_on_batch([z_input, labels_input], y_gan)\n",
    "            # summarize loss on this batch\n",
    "            if (j+1) % 5==0 or dis_loss > 1.10 or gen_loss > 1.10 or gan_loss > 1.80:\n",
    "                diff = int(time.time()-now)\n",
    "                print('>%d/%d, %d/%d, d1=%.3f, d2=%.3f, g=%.3f, secs=%d, tryAgain=%d, nTripsOnSameSavedWts=%d, nSaves=%d' %\n",
    "                    (i+1, epochs_goal, j+1, bat_per_epo, dis_loss, gen_loss, gan_loss, diff, nTryAgains, nTripsOnSameSavedWts, nSaves))\n",
    "            if dis_loss > 0.30 and dis_loss < 0.95 and gen_loss > 0.25 and gen_loss < 0.95 and gan_loss > 0.40 and gan_loss < 1.50:\n",
    "                nTripsOnSameSavedWts = 0\n",
    "                if ij - ijSave > 8:\n",
    "                    nSaves+=1\n",
    "                    ijSave = ij\n",
    "                    d_trainable_weights = np.array(d_model.get_weights())\n",
    "                    g_trainable_weights = np.array(g_model.get_weights())\n",
    "                    gan_trainable_weights = np.array(gan_model.get_weights())\n",
    "            if (dis_loss < 0.001 or dis_loss > 2.0) and ijSave > 0:\n",
    "                nTryAgains+=1\n",
    "                nTripsOnSameSavedWts+=1\n",
    "                print(\"LOADING d_model\",j+1,\" from \",ijSave)\n",
    "                d_model.set_weights(d_trainable_weights)\n",
    "            if (gen_loss < 0.001 or gen_loss > 2.0) and ijSave > 0:\n",
    "                nTryAgains+=1\n",
    "                nTripsOnSameSavedWts+=1\n",
    "                print(\"LOADING g_model\",j+1,\" from \",ijSave)\n",
    "                g_model.set_weights(g_trainable_weights)\n",
    "            if (gan_loss < 0.010 or gan_loss > 4.50) and ijSave > 0:\n",
    "                nTryAgains+=1\n",
    "                nTripsOnSameSavedWts+=1\n",
    "                print(\"LOADING gan_models\",j+1,\" from \",ijSave)\n",
    "                gan_model.set_weights(gan_trainable_weights)\n",
    "            # if (j+1) % 10 == 0:\n",
    "                # summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "            if nTripsOnSameSavedWts > 20:\n",
    "                print(\"**********  Too many rebuilds  **************\")\n",
    "                summarize_performance(i, g_model, d_model, dataset, latent_dim)\n",
    "                import sys\n",
    "                sys.exit(0)\n",
    "        # evaluate the model performance, sometimes\n",
    "        if (i+1) % 5 == 0:\n",
    "            summarize_performance(i, g_model, d_model, gan_model, dataset, latent_dim)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nHOb4pcxjwE5"
   },
   "source": [
    "We can then define the size of the latent space, define all three models, and train them on the loaded face dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yi8_8hCMCmvB"
   },
   "outputs": [],
   "source": [
    "# size of the latent space\n",
    "latent_dim = 100\n",
    "\n",
    "if qRestart:\n",
    "        d_model, g_model, gan_model = restart(epochs_done = epochs_done)\n",
    "else:\n",
    "        # create the discriminator\n",
    "        d_model = define_discriminator()\n",
    "        #d_model = load_model(\"generator_model_dis001.h5\")\n",
    "        # create the generator\n",
    "        #g_model = load_model(\"generator_model_001.h5\")\n",
    "        g_model = define_generator(latent_dim)\n",
    "        # create the gan\n",
    "        gan_model = define_gan(g_model, d_model)\n",
    "        #gan_model = load_model(\"generator_model_gan001.h5\")\n",
    "\n",
    "# load image data\n",
    "dataset, cumProbs = load_real_samples()\n",
    "save_real_plots(dataset, nRealPlots=5)\n",
    "train(g_model, d_model, gan_model,  dataset, latent_dim, epochs_goal=epochs_goal, n_batch=128, epochs_done=epochs_done)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CGAN_Model_TrainerFinal.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
